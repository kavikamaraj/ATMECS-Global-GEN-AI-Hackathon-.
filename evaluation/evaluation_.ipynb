Evaluation Criteria Definition

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.evaluation import load_evaluator
from langchain_openai import ChatOpenAI
from langchain_core.pydantic_v1 import BaseModel, Field
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

# Define a structure for result scoring
class ResultScore(BaseModel):
    score: float = Field(..., description="Score from 0 to 1, where 1 is best.")
    explanation: str = Field(..., description="Explanation for the score.")

# Initialize the language model (GPT-4 in this case)
llm = ChatOpenAI(temperature=0, model_name="gpt-4", max_tokens=4000)


 Evaluation Prompts

# Relevance Evaluation Prompt Template
relevancy_prompt = PromptTemplate(
    input_variables=["question", "contexts"],
    template="""
Q: {question}
Docs: {contexts}

Score each document's relevance to the question:
0.00 - Irrelevant
0.33 - Somewhat relevant
0.66 - Relevant
1.00 - Highly relevant

Final Score: [Average of all scores]
"""
)

relevance_chain = relevancy_prompt | llm.with_structured_output(ResultScore)

# Function to evaluate relevance
def evaluate_relevance(question, contexts):
    result = relevance_chain.invoke({"question": question, "contexts": contexts})
    return result.score, result.explanation


Faithfulness Evaluation

# Faithfulness Evaluation Prompt Template
faithfulness_prompt = PromptTemplate(
    input_variables=["question", "context", "generated_answer"],
    template="""
Question: {question}
Context: {context}
Generated Answer: {generated_answer}

Evaluate if the generated answer can be deduced from the context.
Score 1 if yes, and 0 if no.

Score: 
"""
)

faithfulness_chain = faithfulness_prompt | llm.with_structured_output(ResultScore)

# Function to evaluate faithfulness
def evaluate_faithfulness(question, context, generated_answer):
    result = faithfulness_chain.invoke({"question": question, "context": context, "generated_answer": generated_answer})
    return result.score, result.explanation


Correctness Evaluation

# Correctness Evaluation Prompt Template
correctness_prompt = PromptTemplate(
    input_variables=["question", "ground_truth", "generated_answer"],
    template="""
Question: {question}
Ground Truth: {ground_truth}
Generated Answer: {generated_answer}

Evaluate the correctness of the generated answer based on the ground truth.
Score from 0 to 1, where 1 is perfectly correct.

Score: 
"""
)

correctness_chain = correctness_prompt | llm.with_structured_output(ResultScore)

# Function to evaluate correctness
def evaluate_correctness(question, ground_truth, generated_answer):
    result = correctness_chain.invoke({"question": question, "ground_truth": ground_truth, "generated_answer": generated_answer})
    return result.score, result.explanation

Test the Evaluation Functions

# Test Relevance Evaluation
question = "What is the capital of France?"
contexts = ["Paris is the capital of France.", "The Eiffel Tower is in Paris."]
relevance_score, relevance_explanation = evaluate_relevance(question, contexts)
print(f"Relevance Score: {relevance_score}\nExplanation: {relevance_explanation}")

# Test Faithfulness Evaluation
generated_answer = "Paris"
context = "Paris is the capital of France."
faithfulness_score, faithfulness_explanation = evaluate_faithfulness(question, context, generated_answer)
print(f"Faithfulness Score: {faithfulness_score}\nExplanation: {faithfulness_explanation}")

# Test Correctness Evaluation
ground_truth = "Paris"
correctness_score, correctness_explanation = evaluate_correctness(question, ground_truth, generated_answer)
print(f"Correctness Score: {correctness_score}\nExplanation: {correctness_explanation}")


Combine Evaluations

def overall_evaluation(question, contexts, ground_truth, generated_answer):
    relevance_score, _ = evaluate_relevance(question, contexts)
    faithfulness_score, _ = evaluate_faithfulness(question, contexts[0], generated_answer)
    correctness_score, _ = evaluate_correctness(question, ground_truth, generated_answer)
    
    # Combine the scores (you can adjust the weights as needed)
    final_score = (relevance_score + faithfulness_score + correctness_score) / 3
    return final_score





